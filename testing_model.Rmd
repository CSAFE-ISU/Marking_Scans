---
title: "Untitled"
author: "Andrew Maloney"
date: "11/16/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
 # Set Large Seed.  Seed must be set before loading keras & Tensorflow

#Loading required libraries
library(reticulate)
library(tidyverse)
library(abind)
library(keras)
library(tensorflow)
#install_keras(tensorflow = "gpu") for use with CUDA

use_backend("tensorflow")

reticulate::py_config() #CUDA safety check
reticulate::py_module_available("keras") #CUDA safety check


```

```{r}
# Creating training and validation sets

tensor_x <- abind(array_1_0,
                  array_grooves,
                  array_breakoff,
                  array_damage,
                  array_no_striation,
                  array_1_2,
                  array_vertical_striation,
                  array_1_1,
                  array_1_3,
                  array_grooves_x,
                  array_breakoff_x,
                  array_damage_x,
                  array_vertical_striation_x,
                  array_no_striation_x, along = 1) #bind arrays together to create explanatory data
                                                   #each array was obtained from a cropped (x_resolution by y_resolution) 3d topographical LEA scan data

predictor_y <- c(y_1_0,
                 y_grooves,
                 y_breakoff,
                 y_damage,
                 y_no_striation,
                 y_1_2,
                 y_vertical_striation,
                 y_1_1,
                 y_1_3,
                 y_grooves_x,
                  y_breakoff_x,
                  y_damage_x,
                  y_vertical_striation_x,
                  y_no_striation_x) #bind vectors together to create predictor variables for explanatory data
                                    #each vector was obtained from a best ratio of annotations from each cropped (x_resolution by y_resolution) 3d topographical LEA scan data


validation_x <- abind(array_1_4, along = 1) #Create explanatory data for validation set

validation_y <- c(y_1_4) #Create predictor variables for validation data


predictor_y <- to_categorical(predictor_y) #Encoding for Keras (required) by keras
validation_y <- to_categorical(validation_y) #Encoding for keras (required) by  keras

```

```{r}
#Model using keras function api

create_3dCNN <- function(Learning_rate = 0.001, decay = 0){
  
  k_clear_session()
  #use_session_with_seed(777777, disable_gpu = FALSE, disable_parallel_cpu = FALSE) # set false if you are using CUDA
  tensorflow::tf$random$set_seed(777777)
  
  cnn_model <- keras_model_sequential() %>%
  layer_conv_3d(filters = 32, kernel_size = c(3, 3, 1), input_shape = c(128, 128, 1, 1)) %>%
  layer_zero_padding_3d() %>%
  layer_activation_relu() %>%
  layer_max_pooling_3d(pool_size = c(2, 2, 2)) %>%
  layer_batch_normalization(center = TRUE, scale = TRUE) %>%
  #layer_spatial_dropout_3d(0.5) %>%
  layer_conv_3d(filters = 64, kernel_size = c(3, 3, 1)) %>%
  layer_zero_padding_3d() %>%
  layer_activation_relu() %>%
  layer_max_pooling_3d(pool_size = c(2, 2, 2)) %>%
  layer_batch_normalization(center = TRUE, scale = TRUE) %>%
  #layer_spatial_dropout_3d(0.5) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation_relu() %>%
  layer_dense(units = 6) %>%
  layer_activation("softmax")
  
  cnn_model %>% compile(loss = "categorical_crossentropy",
                        optimizer = optimizer_rmsprop(lr = Learning_rate, decay = decay),
                        metrics = "accuracy"
                        )
  
  return(cnn_model)
  
}

```

```{r}
#determining weights for class imbalance based on majority class

frequency <- c(y_1_0,
                y_grooves,
                y_breakoff,
                y_damage,
                y_no_striation,
                y_1_2,
                y_vertical_striation,
                y_1_1,
                y_1_3,
                y_grooves_x,
                y_breakoff_x,
                y_damage_x,
                y_vertical_striation_x,
                y_no_striation_x)

prop.table(table(y_1_4))

class_0 = max(prop.table(table(frequency)))/as.numeric(prop.table(table(frequency))[1])
class_1 = max(prop.table(table(frequency)))/as.numeric(prop.table(table(frequency))[2])
class_2 = max(prop.table(table(frequency)))/as.numeric(prop.table(table(frequency))[3])
class_3 = max(prop.table(table(frequency)))/as.numeric(prop.table(table(frequency))[4])
class_4 = max(prop.table(table(frequency)))/as.numeric(prop.table(table(frequency))[5])
class_5 = max(prop.table(table(frequency)))/as.numeric(prop.table(table(frequency))[6])

```

```{r}

LogMetrics <- R6::R6Class("LogMetrics",
  inherit = KerasCallback,
  public = list(
    loss = NULL,
    acc = NULL,
    on_batch_end = function(batch, logs=list()) {
      self$loss <- c(self$loss, logs[["loss"]])
      self$acc <- c(self$acc, logs[["accuracy"]])
    }
)) #log the accuracy and loss into the object at the end of each batch used later for cyclical learning rate



```

```{r}
#Defining multiple call_back functions

callback_lr_init <- function(logs){ 
      iter <<- 0
      lr_hist <<- c()
      iter_hist <<- c()
} #clears the learning rate history and iteration history

callback_lr_set <- function(batch, logs){
      iter <<- iter + 1
      LR <- l_rate[iter] # if number of iterations > l_rate values, make LR constant to last value
      if(is.na(LR)) LR <- l_rate[length(l_rate)]
      k_set_value(cnn_model$optimizer$lr, LR)
} #set the learning rate based on l_rate

callback_lr_log <- function(batch, logs){
      lr_hist <<- c(lr_hist, k_get_value(cnn_model$optimizer$lr))
      iter_hist <<- c(iter_hist, k_get_value(cnn_model$optimizer$iterations))
} #logs the learning rate and iteration


callback_lr <- callback_lambda(on_train_begin=callback_lr_init, on_batch_begin=callback_lr_set) #insert into callback_lambda(keras required)
callback_logger <- callback_lambda(on_batch_end=callback_lr_log) #insert into callback_lambda(keras required)


#callbacks_list <- list(callback_learning_rate_scheduler(schedule = scheduler),
                       #callback_early_stopping(monitor = "val_loss", patience = 7),# Save the best Model out of all epochs
                       #callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.1, patience = 10)) # Adjust Learning rate if there is no improvement


```

```{r}
#Determing the learning rate bounds

callback_log_acc_lr <- LogMetrics$new()

cnn_model <- create_3dCNN()

history <- cnn_model %>% fit(
                    tensor_x, predictor_y, 
                    batch_size = batch_size,
                    epochs = epochs_find_LR,
                    shuffle = TRUE,
                    class_weight = list("0" = class_0, "1" = class_1, "2" = class_2, "3" = class_3, "4" = class_4, "5" = class_5),
                    callbacks = list(callback_lr, callback_logger, callback_log_acc_lr),
                    verbose = 2)



```

```{r}
#library(zoo)

#plot accuracy against learning rate curve
plot(lr_hist, callback_log_acc_lr$acc, log="x", type="b", pch=16, cex=0.3, xlab="learning rate (log scale)", ylab="accuracy")

plot(rollmean(lr_hist, 100), rollmean(callback_log_acc_lr$acc, 100), log="x", type="l", pch=16, cex=0.3, xlab="learning rate", ylab="accuracy: rollmean(100)")

Learning_rate_l <- 1.55e-6
Learning_rate_h <- 1.15e-5
plot(rollmean(lr_hist, 100), rollmean(callback_log_acc_lr$acc, 100), log="x", type="l", pch=16, cex=0.3, xlab="learning rate", ylab="accuracy: rollmean(100)")
abline(v=1.15e-6, col="grey60")
abline(v=1.9e-5, col="grey60")
abline(v=Learning_rate_l, col="blue")
abline(v=Learning_rate_h, col="red")

```


```{r}



callback_log_acc_high <- LogMetrics$new()

cnn_model <- create_3dCNN(Learning_rate = Learning_rate_h, decay = 0)

history_2 <- cnn_model %>% fit(
                    tensor_x, predictor_y, 
                    batch_size = batch_size,
                    epochs = 100,
                    validation_data = list(validation_x, validation_y),
                    shuffle = TRUE,
                    class_weight = list("0" = class_0, "1" = class_1, "2" = class_2, "3" = class_3, "4" = class_4, "5" = class_5),
                    callbacks = list(callback_lr, callback_logger, callback_log_acc_lr),
                    verbose = 2)









history_2


```


```{r}



categ <- c("Striations", "Grooves", "Vertical_Striae", "No_Striae", "Damage", "Breakoff")

classes_pred <- cnn_model %>% predict_classes(validation_x)
classes_pred <- categ[as.vector(classes_pred)+1]
classes_test <- categ[apply(validation_y, 1, which.max)]
table(classes_pred)








```